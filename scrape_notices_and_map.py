# -*- coding: utf-8 -*-
"""scrape_notices_and_map.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rbsEOqTQulv3yKziASI-_cWU2-6bFLx

# Installations
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install feedparser
# %pip install requests
# %pip install beautifulsoup4
# %pip install PyGithub
# %pip install numpy
# %pip install dateparser

# Commented out IPython magic to ensure Python compatibility.
# %pip install geopandas
# %pip install pandas
# %pip install matplotlib
# %pip install contextily
# %pip install mapclassify
# %pip install folium

import warnings
warnings.filterwarnings('ignore')

import pandas
import pandas as pd
import geopandas as gpd
import matplotlib
import matplotlib.pyplot as plt
import folium
import contextily
import io
import numpy as np
import mapclassify
import folium
import dateparser as dp
from datetime import timedelta
from datetime import datetime as dt
from calendar import month_name
from shapely.geometry import Point

#Import required packages
from github import Github
from github import InputGitTreeElement
import feedparser
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from bs4 import SoupStrainer
import numpy as np
import io

pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None  # default='warn'

import geopandas.explore
geopandas.explore._MAP_KWARGS += ["dragging","scrollWheelZoom", "maxNativeZoom"]

"""# Scrape newsfeed

NOTE: Github raw url takes 5 mins to update, so might need to wait before running the rest of the code after scraping
"""

#------------------------------------------------------------
# SEARCH FOR WORDS IN STRING
#------------------------------------------------------------
# 'matches' can be existing list or else define as empty list
# before running define 'wordlist' before running

def search_keys(string, wordlist, matches):
  string = string.lower()
  for word in wordlist:
    if word in string:
      matches.append(word)
  return matches

def search_bldgs(string, wordlist):
  for word in wordlist:
    matches = re.findall(word, string)
  return matches

#------------------------------------------------------------
# SCRAPE POST
#------------------------------------------------------------
def scrape_post(entry, df):
  from datetime import datetime as dt
  import dateparser
  import sys

  # GET FEED TITLES & LINKS
  title = entry.title
  link = entry.link

  parsed_title = re.split(r'\s*:\s|(?:\s\-|\-\s)|\(|\)', title)

  # remove elements in list that do not contain letters
  letters = re.compile('[a-zA-Z]+')
  parsed_title = list(filter(lambda x: letters.search(x), parsed_title))

  #filter note
  note = None
  pat1 = r'^[A-Z]+$'
  for i in parsed_title:
    match = re.search(pat1, i)
    if match != None:
      idx = parsed_title.index(i)
      note = parsed_title[idx]
      break

  #filter summary
  summary = None
  pat2 = r'[Bb][uilding]{6,}'
  for i in parsed_title:
    match = re.search(pat2, i)
    if match != None:
      idx = parsed_title.index(i) + 1
      try:
        summary = parsed_title[idx]
      except IndexError:
        pass
      break

  if summary is None:
    summary = ', '.join(parsed_title[:])

  # parse post text
  response = requests.get(link) #get html from post link
  soup = BeautifulSoup(response.content, 'html.parser') #make soup from html
  details = soup.find_all('div', {'class': ['date','rte']})
  fulltext = details[1].get_text() #get full post text

  # get date published
  dateclass = soup.find_all('div', {'class': ['date']})
  pubdate = dateclass[0].string
  pubdate = dateparser.parse(pubdate).date()

  # search post text for regex patterns
  dates = re.findall(r'(\d{4}\-\d{2}\-\d{2})', fulltext) #date format (ex: 2024-02-01)
  times = re.findall(r'(\d{2}:\d{2})', fulltext) #time format (ex: 06:00)
  projectno = re.findall(r'(?:[\D\W])(\d{2}\-\d{3})(?:[\D\W])', fulltext) #construction project number (12-123)
  floor = re.findall(r'([A-Z]+\-?[A-Z]*\d\d?)(?:\D)', fulltext) #floor (ex: H7)
  location = re.findall(r'([A-Z]+\-?\d\d\d\d*\.?\d*)', fulltext) #room number (ex: S115.30)

  # separate start/end dates
  if len(dates)>0:
    startdate = dateparser.parse(dates[0]).date()
  else:
    startdate = None
  if len(dates)>1:
    enddate = dateparser.parse(dates[1]).date()
  else:
    enddate = None

  # separate start/end times
  if len(times)>0:
    starttime = times[0]
  else:
    starttime = None
  if len(times)>1:
    endtime = times[1]
  else:
    endtime = None

  #  SEARCH FOR: buildings
  bldgs = r'\b[\s\W]*([A-Z]+|Hall)[\s\W]*\b'
  building = re.findall(bldgs, title)
  building = ['H' if x=='Hall' else x for x in building]
  while note in building: building.remove(note)

  #  SEARCH FOR: locations
  locations = ['elevator', 'escalator', 'terrace', 'roof', 'door', 'entrance', 'porte', 'loading dock', 'stair', 'mezzanine', 'vestibule', 'lobby', 'tunnel', 'scaffolding',
               'facade', 'sidewalk', 'Mackay', 'Guy St', 'Bishop', 'Catherine', 'Maisonneuve', 'street', 'alley', 'tree', 'ground floor', 'first floor', 'hallway', 'corridor',
               'security desk', 'lounge', 'basement', 'parking']
  location = search_keys(summary, locations, location)

  #  SEARCH FOR: directional location
  direction=[]
  directions = ['north', 'east', 'south', 'west', 'main']
  direction = search_keys(summary, directions, direction)

  new_row = pd.Series({'pubdate': pubdate, 'note': note, 'summary': summary, 'building': list(set(building)), 'floor': list(set(floor)), 'location': list(set(location)), 'direction': list(set(direction)), 'startdate': startdate, 'enddate': enddate, 'starttime': starttime, 'endtime': endtime, 'tags': None, 'fulltext': fulltext, 'link': link})
  new_row = new_row.replace({None: ''}) #replace None with empty string
  page_posts = pd.concat([df, pd.DataFrame([new_row], columns=new_row.index)]).reset_index(drop=True)
  return page_posts

#------------------------------------------------------------
# GET PAGES IN RANGE (based on pagination)
#------------------------------------------------------------
def getpostsinrange(url, firstpage, lastpage, df):
  print(f'Scraping pages {firstpage} to {lastpage} in reverse order...\n--------------------------------------------')

  lastpage = lastpage + 1         #range() starts from zero, so need to add 1
  num_rows = (lastpage - firstpage)*10        #10 rows per page

  posts = df

  for page in reversed(range(firstpage, lastpage)):
    page_posts = pd.DataFrame(columns=('pubdate', 'note', 'summary', 'building', 'floor', 'location', 'direction', 'startdate', 'enddate', 'starttime', 'endtime', 'tags', 'fulltext', 'link'))

    pageurl = url + f'?page={page}'

    #parse feed
    feed = feedparser.parse(pageurl)
    for entry in feed.entries:
      page_posts = scrape_post(entry, page_posts)

    #add page to posts df
    posts =  pd.concat([page_posts, posts]).reset_index(drop=True)

    print(f'Page {page} successfully scraped')

  posts = posts.drop_duplicates(subset=['pubdate', 'fulltext'])
  print('\n----PULL COMPLETE----')
  print('Rows scraped: ', num_rows)
  return posts

#------------------------------------------------------------
# UPDATE CSV IN GITHUB
#------------------------------------------------------------
def updatenotices(filename, df, commit_message =""):
  from datetime import datetime as dt
  import dateparser
  if commit_message == "":
      commit_message = "Last updated - "+ dt.now().strftime('%Y-%m-%d %H:%M:%S')
  notices = df.to_csv(sep=',', index=False)
  g = Github(${{ secrets.SuperSecret }})
  repo = g.get_user().get_repo("concordia-work-notices")
  master_ref = repo.get_git_ref('heads/main')
  master_sha = master_ref.object.sha
  base_tree = repo.get_git_tree(master_sha)
  element_list = list()
  element = InputGitTreeElement(filename, '100644', 'blob', notices)
  element_list.append(element)
  tree = repo.create_git_tree(element_list, base_tree)
  parent = repo.get_git_commit(master_sha)
  commit = repo.create_git_commit(commit_message, tree, [parent])
  master_ref.edit(commit.sha)
  print('Last updated @', dt.now().strftime('%Y-%m-%d %H:%M:%S'))

#------------------------------------------------------------
# COMPILED FUNCTION
#------------------------------------------------------------
def RSS_scraper(url, firstpage, lastpage):
  from datetime import datetime as dt
  import dateparser

  csv_url = "https://raw.githubusercontent.com/treequeen/concordia-work-notices/main/notices.csv"
  # Make sure the url is the raw version of the file on GitHub
  # Remember githraw urls only update every 5 minutes, so need to wait between tests
  download = requests.get(csv_url).content

  # Reading the downloaded content and turning it into a pandas dataframe
  posts = pd.read_csv(io.StringIO(download.decode('utf-8')))
  posts = posts.fillna('')

  posts = getpostsinrange(url, firstpage, lastpage, posts)
  posts = posts.drop_duplicates(subset=['fulltext'])

  print('UPLOAD:\n', posts.loc[1:10, ['pubdate', 'summary']])
  updatenotices('notices.csv', posts)
  print('Total row count: ', len(posts))
  return posts

#------------------------------------------------------------
# RUN SCRAPER
#------------------------------------------------------------
url = 'https://www.concordia.ca/content/concordia/en/offices/facilities/news/_jcr_content/content-main/news_list.xml'
posts = RSS_scraper(url, 1, 3)

#---------------------------------------------------------------------
# RESET CSV !! DELETE ALL DATA !! (for starting over and re-scraping)
#---------------------------------------------------------------------
#posts = pd.DataFrame(columns=('pubdate', 'note', 'summary', 'building', 'floor', 'location', 'direction', 'startdate', 'enddate', 'starttime', 'endtime', 'tags', 'fulltext', 'link'))
#updatenotices('notices.csv', posts)

"""# Shapefiles"""

#------------------------------------------------------------
# FUNCTION TO READ IN SHAPEFILES FROM RAW GITHUB URL
#------------------------------------------------------------
def get_shp(filename):
  shp = gpd.read_file(f'/vsicurl/https://raw.githubusercontent.com/treequeen/concordia-work-notices/shapefiles/{filename}.shp')
  return shp

#------------------------------------------------------------
# FUNCTION TO REFORMAT POINT FEATURE ATTRIBUTES
#------------------------------------------------------------
def format_pts(shp, col, bldg_code):
  import re
  shp[col] = shp[col].str.replace(bldg_code, '')

  #remove spaces, zeroes, etc
  shp[col] = shp[col].str.replace(r'[\s\/\\\(\)[A-Z]]*', '', regex=True)
  shp[col] = shp[col].str.replace('.00', '')
  shp[col] = shp[col].str.replace('.01', '')
  shp[col] = bldg_code + shp[col].astype(str)  #add building code
  shp[col] = shp[col].str.replace('Hterrace', 'terrace')
  shp[col] = shp[col].str.replace('Hstairs', 'stair')
  shp[col] = shp[col].str.replace('Hescalator', 'escalator')
  shp[col] = shp[col].str.replace('Helevator', 'elevator')
  shp[col] = shp[col].str.replace('Hentrance', 'entrance')
  shp.rename(columns={f'{col}': 'location'}, inplace=True)  #rename col
  shp['DocName'] = shp['DocName'].str.replace('.dwg', '') #remove ext
  shp.rename(columns={'DocName': 'floor'}, inplace=True)  #rename col
  shp = shp[['location', 'floor', 'geometry']]
  shp.loc[shp['floor'] == 'H1M' , 'location'] = shp.loc[shp['floor'] == 'H1M', 'location'].str.replace('H1', 'H00')
  mask = shp.floor.isnull()
  shp.location[mask] = shp.location[mask].str.replace('H1', 'H00')
  shp['location'] = shp['location'].str.replace(r'-\d+', '', regex=True)
  shp['floor'] = shp['floor'].str.replace('H1M', 'H00')
  shp.floor[mask] = 'H00'
  return shp

#------------------------------------------------------------
# PREPARE POINT/POLYLINE SHAPEFILES
#------------------------------------------------------------
#points
points = get_shp('points')
points = format_pts(points, 'RefName', 'H')
points = points.to_crs(epsg=4326)
display(points)

#polylines
polylines = get_shp('Polylines_new')
polylines = polylines.to_crs(epsg=4326)
polylines.rename(columns={'RefName': 'location', 'DocName': 'floor'}, inplace=True)  #rename col
polylines['location'] = polylines['location'].str.replace('stairs', 'stair')
polylines = polylines.assign(centroid=list(polylines.centroid))
polylines['centroid'] = polylines.centroid
polylines['direction'] = np.nan

#------------------------------------------------------------
# FUNCTION TO GET DIRECTION AND CLOSEST STREET
#------------------------------------------------------------
#Identify directional zone and closest street.
#need to do this for some points as there are multiple entrances and no way to differentiate.
#for example, if you skip this step all entrances would light up even if only one entrance is closed.

def getzonestreet(df):

  midlat = df.geometry.x.mean()
  midlon = df.geometry.y.mean()

  # Define the conditions and corresponding categories
  conditions = [
    (df.centroid.x < midlat) & (df.centroid.y > midlon),
    (df.centroid.x < midlat) & (df.centroid.y < midlon),
    (df.centroid.x > midlat) & (df.centroid.y > midlon),
    (df.centroid.x > midlat) & (df.centroid.y < midlon),
  ]

  dcategories = ['north, west', 'south, west', 'north, east', 'south, east']

  # Use np.select() to create the new column
  df.insert((len(df.columns) - 1), 'direction', pd.Series())
  df['direction'] = np.select(conditions, dcategories, default=None)

  scategories = ['alley', 'mackay', 'bishop', 'maisonneuve']

  # Use np.select() to create the new column
  df.insert((len(df.columns) - 1), 'tags', pd.Series())
  df['tags'] = np.select(conditions, scategories, default=None)

  # Print the resulting dataframe
  display(df)
  return df

# RUN FUNCTION
getzonestreet(points)

"""# Notice data"""

#------------------------------------------------------------
# FUNCTION TO SEARCH POST FOR ADDITIONAL TAGS
#------------------------------------------------------------
def search_tags(string, string2):
  tagged = []
  string = string.lower()
  string2 = string2.lower()
  mytags = ['unplanned', 'update', 'cancel', 'commence', 'begin', 'start', 'reopening', 'interior', 'exterior', 'closed', 'closure', 'main',
            'out of service', 'noise','noisy', 'terrace', 'sidewalk', 'alley', 'mackay', 'mackay', 'guy', 'bishop', 'catherine', 'maisonneuve', 'water', 'hydro',
            'power', 'electric', 'men\'s', 'women\'s','washroom', 'bathroom', 'restroom', 'ventilation', 'air' 'a/c', 'exhaust', 'replace', 'parking',
            'interruption', 'inspection', 'exploratory', 'collection', 'exterior', 'outside', 'scaffolding', 'repair', 'renovation', 'fire alarm', 'test']

  for word in mytags:
    if word in string:
      tagged.append(word)
    if word in string2:
      tagged.append(word)

  tagged = ', '.join(list(set(tagged)))
  return tagged

#------------------------------------------------------------
# FUNCTION TO READ IN NOTICE DATA FROM RAW GITHUB URL AS DF
#------------------------------------------------------------
def csv2df(csv_url):
  download = requests.get(csv_url).content
  df = pd.read_csv(io.StringIO(download.decode('utf-8')))
  df = df.fillna('')
  return df

#------------------------------------------------------------
# FUNCTION TO FORMAT LOCATION/FLOOR TO MATCH SHP
#------------------------------------------------------------
def format_locs(df):
  # Explode and format room numbers
  df = df.explode('location') #duplicate notice row for each room
  df.location = df.location.str.replace(r'\.*$', '', regex=True)
  df.location = df.location.str.replace(r'-*', '', regex=True)
  df.drop_duplicates(subset=['pubdate', 'location', 'fulltext'], inplace=True)
  df.reset_index(drop=True, inplace=True)

  # Get floors from room number
  df.floor = df.location.str.extract(r'(H[23456789])\d', expand=False)
  df = df.fillna('')
  #df['New Column'] = np.where(df.columnA.str.match(regex), True, False)
  df.floor = np.where(df.location.str.match(r'H00\d+'), 'H00', df.floor)
  df['floor'] = np.where(df.location.str.match(r'H1\d{2}'), 'H1', df.floor)
  df['floor'] = np.where(df.location.str.match(r'H12\d{2}'), 'H12', df.floor)
  df.floor = np.where(df.location.str.match(r'H10\d{2}'), 'H10', df.floor)
  df['floor'] = np.where(df.location.str.match(r'H11\d{2}'), 'H11', df.floor)
  return df

# PULL NOTICES CSV FROM GITHUB
url = 'https://raw.githubusercontent.com/treequeen/concordia-work-notices/main/notices.csv'
notices = csv2df(url)
from ast import literal_eval
notices.location = notices.location.apply(literal_eval) #convert to list type

# GENERATE TAGS
for i in range(0, len(notices)):
  tagged = search_tags(notices.summary[i], notices.fulltext[i])
  notices.tags[i] = tagged

# FORMAT LOCATIONS
notices = format_locs(notices)

# SUBSET HALL BUILDING NOTICES
h_notices = notices[notices['building'].apply(lambda x: 'H' in x)]
print(len(h_notices))
h_notices.head()

"""# Format times"""

#------------------------------------------------------------
# FUNCTION TO SEARCH POST FOR APPROX. END MONTH
#------------------------------------------------------------
# Exact dates are formatted consistently, BUT for longer term construction projects where end dates
# are not exact, post normally states (example) "...project will be completed by May 2025."
# Below function searches for end months for longer term projects.

def get_endmonth(df, i):
  pubyear = str(dp.parse(str(df.pubdate.iloc[i])).year)
  date = None
  y = None
  notice = df.fulltext.iloc[i].title()

  for m in month_name[1:]:
    if m in notice:
      date = m + ' 1'

  y = re.match(r'/s202/d/s', notice)
  if y:
    date = date + ' ' + y.group(0)
  elif date is None:
    pass
  else:
    date = date + ' ' + pubyear

  if date is None:
    pass
  else:
    dateobj = dp.parse(date)
    return dateobj

#------------------------------------------------------------
# FUNCTION TO CALCULATE PROJECT DURATION
#------------------------------------------------------------
def get_dur(df, i):
  d1 = df.start.iloc[i]
  d2 = df.end.iloc[i]
  diff = d2 - d1
  timespan = int(diff.total_seconds())
  hours = int(round(timespan/3600, 0))
  if hours > 730:
    duration = f'{round(hours/730, 1)} months' # get months
  elif hours > 168:
    duration = f'{round(hours/168, 1)} weeks' # get weeks
  elif hours > 24:
    duration = f'{round(hours/24)} days' # get days
  else:
    duration = f'{hours} hours'
  return duration, timespan

#-------------------------------------------------------------------------
# FUNCTION TO FORMAT START/END AS DATETIME; CALCULATE PROJECT DURATION
#-------------------------------------------------------------------------
# Requires columns: startdate, enddate, starttime, endtime & pubdate (used to replace empty startdates)

def format_times(df):
  for i in range(0, len(df)):

    #fill empty startdate with date published
    if df.startdate.iloc[i] == "":
      df.startdate.iloc[i] = df.pubdate.iloc[i]
    if df.enddate.iloc[i] == "":
      endmonth = get_endmonth(df,i)
      if endmonth is None:
        df.enddate.iloc[i] = df.startdate.iloc[i]
      elif endmonth <= dp.parse(str(df.startdate.iloc[i])):
        df.enddate.iloc[i] = df.startdate.iloc[i]
      else:
        df.enddate.iloc[i] = endmonth

    #fill None entries for start date
    if df.startdate.iloc[i] is None:
      df.startdate.iloc[i] = df.pubdate.iloc[i]

    #fill None entries for end date
    if df.enddate.iloc[i] is None:
      endmonth = get_endmonth(df, i)
      if endmonth is None:
        df.enddate.iloc[i] = df.startdate.iloc[i]
      elif endmonth <= dp.parse(str(df.startdate.iloc[i])):
        df.enddate.iloc[i] = df.startdate.iloc[i]
      else:
        df.enddate.iloc[i] = endmonth

    #convert to datetime obj
    df.startdate.iloc[i] = dp.parse(str(df.startdate.iloc[i]))
    #convert to date
    df.startdate.iloc[i] = df.startdate.iloc[i].date()

    #convert to datetime obj
    df.enddate.iloc[i] = dp.parse(str(df.enddate.iloc[i]))
    #convert to date
    df.enddate.iloc[i] = df.enddate.iloc[i].date()

    df.starttime.iloc[i] = dp.parse(str(df.starttime.iloc[i]))
    #fill None entries for start time
    if df.starttime.iloc[i] is None:
      df.starttime.iloc[i] = '00:00'
      df.endtime.iloc[i] = '23:59'
    df.starttime.iloc[i] = dp.parse(str(df.starttime.iloc[i]))

    df.endtime.iloc[i] = dp.parse(str(df.endtime.iloc[i]))
    #fill None entries for end time
    if df.endtime.iloc[i] is None:
      df.endtime.iloc[i] = '23:59'
    df.endtime.iloc[i] = dp.parse(str(df.endtime.iloc[i]))

    #convert to time obj
    df.starttime.iloc[i] = df.starttime.iloc[i].time()

    #convert to time obj
    df.endtime.iloc[i] = df.endtime.iloc[i].time()

  #add empty start and end columns to point layer
  df['start'] = pd.Series()
  df['end'] = pd.Series()
  df.insert(11, "timespan", pd.Series())
  df.insert(12, "Duration", pd.Series())

  for i in range(len(df)):

    #combine date and times to one datetime
    df.start.iloc[i] = dt.combine(df.startdate.iloc[i], df.starttime.iloc[i])
    df.end.iloc[i] = dt.combine(df.enddate.iloc[i], df.endtime.iloc[i])

    #calculate duration
    duration, timespan = get_dur(df, i)
    df.timespan.iloc[i] = timespan
    df.Duration.iloc[i] = duration

  return df

# RUN FUNCTION
h_notices = format_times(h_notices)
h_notices.head()

"""# Unformat times

Geopandas returns error when datetime columns are in the gdf, so need perform the operation to check whether notice is past, planned, or active, mark type, then convert back to string. Also, drop start and end column as it is no longer needed.
"""

#------------------------------------------------------------
# FUNCTION TO CHECK IF PROJECT IS PAST, ACTIVE, OR PLANNED
#------------------------------------------------------------
def checkstatus(df):
  now = dt.now()
  df['status'] = pd.Series()
  for i in range(len(df)):
    if df.start.iloc[i] <= now <= df.end.iloc[i]:
      df['status'].iloc[i] = 'Active'
    elif now >= df.end.iloc[i]:
      df['status'].iloc[i] = 'Past'
    elif now <= df.start.iloc[i]:
      df['status'].iloc[i] = 'Planned'
    else:
      df['status'].iloc[i] = ''
  return df

#------------------------------------------------------------
# FUNCTION TO REFORMAT DATETIME AS STRING
#------------------------------------------------------------
def unformat_times(df):
  df.drop(['start', 'end'], axis=1, inplace=True)
  for i in range(len(df)):
    df.startdate.iloc[i] = df.startdate.iloc[i].strftime("%A %b. %-d, %Y")
    df.enddate.iloc[i] = df.enddate.iloc[i].strftime("%A %b. %-d, %Y")
    df.starttime.iloc[i] = df.starttime.iloc[i].strftime("%-I:%M %p")
    df.endtime.iloc[i] = df.endtime.iloc[i].strftime("%-I:%M %p")
  return df

# RUN FUNCTIONS
h_notices = checkstatus(h_notices)
h_notices = unformat_times(h_notices)

h_notices.head()

"""# Join data"""

#------------------------------------------------------------
# FUNCTION TO GENERATE RANDOM POINTS IN BUILDING
#------------------------------------------------------------
def random_nearbypts(gdf, length):
  points = []
  minx = gdf.geometry.x.max()
  miny = gdf.geometry.y.min()
  maxx = gdf.geometry.x.max()
  maxy = gdf.geometry.y.max()
  while len(points) < length:
    pnt = Point(np.random.uniform(minx, maxx), np.random.uniform(miny, maxy))
    points.append(pnt)
  return points

#------------------------------------------------------------
# FUNCTION TO FILL GEOMETRY NOT MATCHED BY MERGE
#------------------------------------------------------------
# This process may be able to be optimized by updating the RSS scraper and spatial data.
# The function also creates new points for exterior work.
# Remaining geometry is given a random point

def fillemptygeoms(shp, gdf):

  # Define geoms for entrances by subsetting shapefiles
  main_ent = shp.loc[(shp['location'] == 'entrance') & (shp['tags'] == 'maisonneuve'), 'geometry']
  mackay_ent = shp.loc[(shp['location'] == 'entrance') & (shp['tags'] == 'mackay'), 'geometry']
  bishop_ent = shp.loc[(shp['location'] == 'entrance') & (shp['tags'] == 'bishop'), 'geometry']
  terrace = shp.loc[(shp['location'] == 'terrace') & (shp['floor'] == 'H1'), 'geometry']
  washrooms = shp.loc[(shp['location'] == 'H502') | (shp['location'] == 'H510'), 'geometry']

  # Exteriors based on entrance geoms
  maisonn_ext = main_ent.translate(xoff=0.00005, yoff=-0.00005, zoff=0.0).iloc[0]
  mackay_ext = mackay_ent.translate(xoff=-0.00005, yoff=-0.00005, zoff=0.0).iloc[0]
  bishop_ext = bishop_ent.translate(xoff=0.00005, yoff=0.00005, zoff=0.0).iloc[0]
  alley = terrace.translate(xoff=-0.00005, yoff=0.00005, zoff=0.0).iloc[0]

  # Assign geoms to notices by subsetting gdf
  gdf[(gdf['location'] == 'entrance') & (gdf['tags'].isin(['maisonneuve']))] = main_ent.iloc[0]
  gdf.loc[(gdf['location'] == 'entrance') & (gdf['tags'].isin(['mackay']))] = mackay_ent.iloc[0]
  gdf.loc[(gdf['location'] == 'entrance') & (gdf['tags'].isin(['bishop']))] = bishop_ent.iloc[0]
  gdf.loc[(gdf['tags'].isin(['terrace']))] = terrace.iloc[0]

  # Assign geoms by identifying notices about exterior work
  exttags = ['sidewalk', 'exterior', 'outside', 'scaffolding']
  gdf.loc[(gdf['tags'].isin(['maisonneuve']))&(gdf['tags'].isin(exttags))] = maisonn_ext
  gdf.loc[(gdf['tags'].isin(['mackay']))&(gdf['tags'].isin(exttags))] = mackay_ext
  gdf.loc[(gdf['tags'].isin(['bishop']))&(gdf['tags'].isin(exttags))] = bishop_ext
  gdf.loc[(gdf['tags'].isin(['alley']))&(gdf['tags'].isin(exttags))] = alley

  mask = gdf.geometry.isnull()
  #Generate random points
  length = mask.sum()
  randpts = random_nearbypts(gdf, length)
  gdf.geometry[mask] = randpts

  return gdf

# MERGE SHPS WITH NOTICES, DROP ROWS WITH NULL GEOMS
pts  = gpd.GeoDataFrame(h_notices.merge(points, on='location', how='left', suffixes=('', '_y')))
pts.drop(['floor_y', 'direction'], axis=1, inplace=True)

null = pts.geometry.isna()
nullgeoms = pts[null]
filledgeoms = fillemptygeoms(points, nullgeoms)

pts.dropna(subset=['geometry'], inplace=True)
pts = gpd.GeoDataFrame(pd.concat([pts, nullgeoms], ignore_index=True), crs=points.crs)

pts = pts.dissolve(by=list(pts.columns[0:10]), as_index=False)
pts = pts.drop([0])
pts = pts.iloc[::-1].reset_index(drop=True)

#pts.geometry = geopandas.points_from_xy(x=pts.geometry.x, y=pts.geometry.x)
pts.set_geometry('geometry', inplace=True)

# Reorder
col = pts.pop('geometry')
pts.insert(18, col.name, col)
col = pts.pop('starttime')
pts.insert(8, col.name, col)

# Convert multipoints to single point
pts['geometry'] = pts.representative_point()

# Show length
print(len(pts))
pts.head(3)

"""# Map"""

# Rename columns for tooltip (can also use aliases)
pts.rename(columns={'summary': 'Title', 'floor': 'Floor', 'location': 'Location', 'startdate': 'Start date', 'enddate': 'End date', 'starttime': 'Start time', 'endtime': 'End time', 'tags': 'Tags', 'status': 'Status'}, inplace=True)

# Subset by status
active = pts[pts['Status'] == 'Active']
past = pts[pts['Status'] == 'Past']
planned = pts[pts['Status'] == 'Planned']

#--------------------------------------------------------------------------
# FUNCTION TO JITTER POINTS (Ended up not using this but can be useful)
#--------------------------------------------------------------------------
def jitter_pts(gdf):
  for i in range(0, len(gdf)):
    xjitter = np.random.uniform(-0.00002,0.00002)
    yjitter = np.random.uniform(-0.00002,0.00002)
    mask = gdf.geom_almost_equals(gdf.geometry.iloc[i], decimal=5)
    gdf.geometry[mask] = gdf.translate(xjitter, yjitter)
  return gdf

#active = jitter_pts(active)
#past = jitter_pts(past)
#planned = jitter_pts(planned)

#--------------------------------------------------------------------------
# ADD LAYERS TO BASEMAP   # Requires 'basemap'
#--------------------------------------------------------------------------
# namestr: Name of layer in Layer Control
# showTF: True/False to show layer on default

def addlayer(gdf, namestr, showTF):
  gdf.explore(
      m=basemap,  # pass the map object
      column="timespan",  # make choropleth based on column
      marker_kwds=dict(radius=30, fill=True),  # make marker radius 10px with fill
      style_kwds=dict(stroke=True, weight=20, opacity=0.15, fillOpacity=0.5),  # make marker radius 10px with fill
      cmap='autumn_r',
      scheme='Quantiles',
      k=20,
      categorical=False,
      tooltip=['Title', 'Floor', 'Location', 'Start date', 'Start time', 'End date', 'End time', 'Duration', 'Tags', 'Status'],
      legend=False,
      name=namestr,
      popup=['Title', 'Floor', 'Location', 'Start date', 'Start time', 'End date', 'End time', 'Duration', 'Status','fulltext', 'link'],
      popup_kwds=dict(parse_html=True, max_width=700),
      show=showTF
  )

import folium
from branca.colormap import LinearColormap

# Create colorbar legend
colormap = LinearColormap(colors=['yellow', 'red'],vmin=0, vmax=1,max_labels=1)

# If not using features in base layer, use this to create a central point
#base = points.dissolve().centroid

# Create basemap
basemap = polylines.explore(
    tiles="CartoDB positron",
    name='Show interior outlines',
    color="#a6abae",
    style_kwds=dict(weight=1),
    zoom_start=21,
    zoom_control=True,
    dragging=True,
    scrollWheelZoom=False,
    attr=''
)

# Add layers
m = addlayer(active, 'Show active work', True)
addlayer(planned, 'Show planned work', True)
addlayer(past, 'Show past work', False)

# Add layer control
folium.LayerControl(collapsed=False).add_to(basemap)  # use folium to add layer control

# Add colorbar
colormap.caption = "Duration of project"
basemap.add_child(colormap)

# Save the map as HTML
basemap.save('noticemap.html')
