# -*- coding: utf-8 -*-
"""RSS-scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qquq6qzlIGYwsFWZwf12Gi3dfeRX-ppO

# Installations
"""

!pip install feedparser
!pip install requests
!pip install beautifulsoup4
!pip install pandas
!pip install PyGithub
!pip install numpy

#Import required packages
from github import Github
from github import InputGitTreeElement
from datetime import datetime
import feedparser
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from bs4 import SoupStrainer
import numpy as np

"""# Functions"""

#function to search 2 strings for keywords

def search_keys(string1, string2):
  global keywords
  keywords = []
  string1 = string1.lower()
  string2 = string2.lower()
  wordlist1 = ['elevator', 'escalator']
  wordlist2 = ['commence', 'begin', 'start', 'reopening', 'terrace', 'roof', 'parking', 'building', 'entrance', 'loading dock',
    'freight elevator', 'interior', 'exterior', 'staircase', 'corridor', 'mezzanine', 'closed', 'closure', 'out of service', 'noise',
    'noisy', 'north', 'east', 'south', 'west', 'vestibule', 'lobby', 'ground floor', 'hallway', 'security desk']

  for word in wordlist1:
    if word in string1:
      keywords.append(word)

  for word in wordlist2:
    if word in string2:
      keywords.append(word)

#function to create df
def create_df():
  global posts
  #create empty dataframe
  posts = pd.DataFrame(columns=('pubdate', 'summary', 'building', 'floor', 'rooms', 'startdate', 'enddate', 'starttime', 'endtime', 'keywords', 'bullets', 'fulltext', 'link'))

#function to scrape post text
def scrape_post(entry):
  from datetime import datetime
  global pubdate, summary, building, floor, rooms, startdate, enddate, starttime, endtime, keywords, info, fulltext, link
  global posts
  global new_row

  #get titles and links
  title = entry.title
  link = entry.link

  #parse title
  pattern = r'(?:\w+[-:\s])?(\w+[/\s]*\w+)\sBuilding[s\s]*:\s([^\(\-]+)'
  parsed_title = re.search(pattern, title)
  building = parsed_title[1]
  summary = parsed_title[2]
  if building == 'Hall':  #change Hall to H
    building = 'H'

  #parse post text
  response = requests.get(link) #get html from post link
  soup = BeautifulSoup(response.content, 'html.parser') #make soup from html
  details = soup.find_all('div', {'class': ['date','rte']})
  fulltext = details[1].get_text() #get full post text

  #get date published
  dateclass = soup.find_all('div', {'class': ['date']})
  pubdate = dateclass[0].string
  pubdate = datetime.strptime(pubdate, '%B %d, %Y')
  pubdate = pubdate.strftime("%x")

  #search for regex patterns
  dates = re.findall(r'(\d{4}\-\d{2}\-\d{2})', fulltext) #date format (ex: 2024-02-01)
  times = re.findall(r'(\d{2}:\d{2})', fulltext) #time format (ex: 06:00)
  projectno = re.findall(r'(?:[\D\W])(\d{2}\-\d{3})(?:[\D\W])', fulltext) #construction project number (12-123)
  floor = re.findall(r'([A-Z]+\-?[A-Z]*\d\d?)(?:\D)', fulltext) #floor (ex: H7)
  rooms = re.findall(r'([A-Z]+\-?\d\d\d\d*\.?\d*)', fulltext) #room number (ex: S115.30)

  #separate start/end dates
  startdate = dates[0]
  if len(dates)>1:
    enddate = dates[1]
  else:
    enddate = None

  #separate start/end times
  if len(times)>0:
    starttime = times[0]
  else:
    starttime = None
  if len(times)>1:
    endtime = times[1]
  else:
    endtime = None

  #extract bullets under "general info"
  all_ul = details[1].find_all('li') #get <li></li> items
  bullets = all_ul[1:] #get bullets (2nd-last <li> objects)
  info = []
  for bullet in bullets:
    info.append(bullet.string)

  #get keywords
  search_keys(title, fulltext)

  #function to add new entry to df
  def append_row(df, row):
    return pd.concat([df, pd.DataFrame([row], columns=row.index)]).reset_index(drop=True)

  #function to check if series is in dataframe
  def match_exists(df, series):
    return (df == series).any().all() #return list of row matches (False for non-match), then return True if all values are true (Row is a match), False if any values returned false (Row is not a match)

  #create new row
  new_row = pd.Series({'pubdate': pubdate, 'summary': summary, 'building': building, 'floor': floor, 'rooms': rooms, 'startdate': startdate, 'enddate': enddate, 'starttime': starttime, 'endtime': endtime, 'keywords': keywords, 'bullets': info, 'fulltext': fulltext, 'link': link})
  new_row = new_row.replace({None: ''}) #replace None with empty string

  if len(posts) == 0:
    posts = append_row(posts, new_row)
  else:
    if match_exists(posts, new_row): #check if row matches any in df
      pass #if match found, do nothing
    else: #if no match found, add new row
      posts = append_row(posts, new_row)
      num_newrows = num_newrows + 1

def updatenotices(filename, notices, token, repo, commit_message =""):
    if commit_message == "":
       commit_message = "Last updated - "+ datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    token = Github(token)
    repo = token.get_repo(repo)
    master_ref = repo.get_git_ref('heads/main')
    master_sha = master_ref.object.sha
    base_tree = repo.get_git_tree(master_sha)
    element_list = list()
    element = InputGitTreeElement(filename, '100644', 'blob', notices)
    element_list.append(element)
    tree = repo.create_git_tree(element_list, base_tree)
    parent = repo.get_git_commit(master_sha)
    commit = repo.create_git_commit(commit_message, tree, [parent])
    master_ref.edit(commit.sha)
    print('Last updated @', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

def RSS_scraper(url):
  from datetime import datetime
  global posts

  if 'posts' not in globals():
    create_df()

  #parse feed
  feed = feedparser.parse(url)

  num_newrows = 0
  for entry in reversed(feed.entries):
    scrape_post(entry)

  #list files to upload and desired file names with which you want to save on GitHub
  notices = posts.to_csv(sep=',', index=False)
  filename = 'notices.csv'

  #Create connection with repo
  token = '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'
  repo = 'treequeen/concordia-work-notices'

  updatenotices(filename, notices, token, repo)
  print('New rows added: ', num_newrows)
  print('Total row count: ', len(posts))

#enter feed url
url = "https://www.concordia.ca/content/concordia/en/offices/facilities/news/_jcr_content/content-main/news_list.xml"

RSS_scraper(url)

